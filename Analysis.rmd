---
title: "Analysis of the Hungarian real estate market"
output:
  html_document:
    df_print: kable
    number_sections: yes
    toc: yes
---




#Be aware, most of this document had been written late at night and hadn't been updated - or, frankly, proofread - in a long-long time to the point that it cannot be considered anything more than a draft. Thank you for your understanding 

#Introduction

##Motivation

In this document, I will attempt to analyse the real estate market using the data collected from the largest Hungarian classified ads website jofogas.hu

The motivation for this analysis is to serve threefold:It is primarily an educational project for building skills in web-scraping, data analysis and machine learning which should be equally useful as a presentation of the authors skills during a possible job hunt. The third reason for creating this document is to use the collected information for evaluating offers.

The initial  question I am attempting to answer here is "Using the data acquired via the crawler, what algorithm can serve as an accurate enough model for predicting the ask price of these condiniums. Conversely, which algorithm describes the advertisement period of these apartments. 

##Research Question
I did not have time to accurately address the following questions: 

An additional question would be: instead of taking snapshots of the data, define this dataset as a time series would allow us to make any conclusions.

Further more, can we use machine learning to identify mortgage claim peddlers? 

Can we use classification tasks to identify ads of the same property? 


##Limitations, technical background

Jofogas.hu was founded in 2010 and got subsequently purchased by the Norwegian  Schibsted Media Group which one quasi monopoly over the Hungarian classified ads market when, in 2015, purchased its local rival, OLX.hu.

In addition to using only one site not every site operating on the market, the scope of subject was also limited to the condinium type of offers posted in the capital of Hungary, Budapest. 

The reason for using jofogas.hu instead of ingatlan.com with its twice as large database comes down to some embarrassingly technical reasons. The night I have started this project was also the night ingatlan.com was down.  Which is a problem because apart from a twice as large ad population, ingatlan.com is forcing a rather rigid postal address information table on a the would-be sellers which means that the location data collected from them is tidy (compete and accurate). The silver lining is that jofogas.hu in addition to being a general ad site covering a wide array of markets, the cocmpany also manages the largest second hand automobile online marketplace hasznaltauto.hu which in turn raises the possibility for the code produced in this exercise to be adopted and adapted and repurposed for and by other projects.
The freetext-produced location data, however, is going to be a rather huge problem. I doubt anyone needs to be reminded of the tricolon describing the ultimate maxim on the real estate market: "Location, ocation,location!"

The data was gathered using web crawler I have written in R using the rvest package. The script is capable of handling multiple projects, designed to automatically update the local database daily. Aside from the published attributes, the script retroactively calculates the expiration date from the last two sampling in which between the ad stopped showing up.

Due its incremental nature, this research grants us the ability to measure the length of the advertisements and any changes in the ad population. Unfortunately, as of 2017-12-30, there are no mechanics implemented for identifying ads by multiple users. 

The data analysis was performed using R packages, primarily by the caret package which offers a uniform a interface for doing predictive analysis on several hundreds of packages.

At some parts,mostly where the larger data.frames are handled, I have opted for using the sqldf package in conjunction with SQLite over the dplyr or base R in anticipation of the need to offload some of the database management tasks to a proper SQL server.

For this analysis, J am going to use the data collected with session ID:
JofogasLakasokBPFull_20180101_23295P


#Loading the data and packages

```{r setup, results='hide',message=FALSE}
library(futile.logger)
flog.threshold(TRACE)
if(!exists("JobLogFile")){JobLogFile<-"Analysis.log"}

flog.appender(appender.tee(JobLogFile))
flog.info("----- NEW EXECUTION STARTED -----")
library(caret)
library(Amelia)
library(sqldf)
library(dplyr)
library(reshape2)
library(randomForest)
library(gridExtra)
library(modelr)
library(stringi)
library(lubridate)

#library(RWeka)


libraries<-c("caret","sqldf","Amelia","sqldf","dplyr","reshape2","randomForest","gridExtra","modelr","stringi")
for(index in 1:length(libraries)){
   lapply(libraries, require, character.only = TRUE)
}

GoFULL<-FALSE
set.seed(7)
LocationData=FALSE
ZipData=TRUE
DOsvm=FALSE
Scope<-"Total"
```

Loading the session and general session information.
EDIT: This section had been updated to allow trendcalculations. Previously,the link between the index of collected ads and the ad details were calculated on the fly using the scans runtime and individual a ad's snapshot for each selected snapshot. This can be adjusted to grant these calculations prior to selecting the session. This will consolidate the master index to be using the composite key of SiteID + querytime. It makes sense to move this preprocessing to the session script. Or perhaps into a separate script. 
```{r loading_the_data}
flog.info("STARTED: loading_the_data")
AnalysisSessionID<-"JofogasLakasokBPFull_20180106_2236JF"
AnalysisAdListFile<-"FullBudapestAdLists.Rda"
AnalysisAdDetailListFile<-"Detailedhirdetes.Rda"

SessionTableFile<-"SessionTable.Rda"
AnalysisSessionTable<-readRDS(file=SessionTableFile, refhook =NULL)
AnalysisSessionRecord<-AnalysisSessionTable[AnalysisSessionTable$SessionID==AnalysisSessionID,]
ForShowTable<-t(AnalysisSessionRecord)
colnames(ForShowTable)<-c("Value")
knitr::kable(ForShowTable,format="html", pad=0)

QueryEndTime<-AnalysisSessionRecord$QueryAdDetailEndTime
AnalysisAdList<-readRDS(file=AnalysisAdListFile, refhook =NULL)
AnalysisAdDetailList<-readRDS(file=AnalysisAdDetailListFile, refhook =NULL)
#AnalysisAdList<-AnalysisAdList[AnalysisAdList$SessionID==AnalysisSessionID,'SiteID']
#AnalysisAdDetailList<-AnalysisAdDetailList[AnalysisAdDetailList$SiteID %in% AnalysisAdList,]
n_occur<-data.frame(table(AnalysisAdDetailList$SiteID))
DuplicateRows<-nrow(n_occur[n_occur$Freq > 1,])
rm(n_occur)
flog.info("FINISHED: loading_the_data")
```

#Exploratory Data Analysis

##Duplicate values

Checking for duplicates reveals *`r DuplicateRows `* a major error in the data collection process:In order to be able to track changes in price, the cawler stores postings whose prices have altered as additional records. Since the crawler scan use the siteid as keys, the queries return all the instances of an ad.. 

Since writing the following paragraph, I have realised that I could have used the SiteID+Price as composite keys and thus render it redundant regarding the problem discussed above, it does serve as a working solution for eliminating duplicates created for any other reason thereby increasing the robustness of the analysis. 

In order to combat duplication , we could grab either the latest entry, or we could go for the entry that was online at the time of taking the snapsot. The latter has the advantage of rendering the data 'immutable', fixed.  To identify the relevant records, I cannot rely on any SessionIds as unchanged records would carry a prior scans' IDs. Luckily we have the timestamp of finishing the query and the query date of each record stored, with these piece of data, we can find the records that were created last or closest to the time of the snapshot while preceeding it.  Who knew playing safe and documenting everything could prove a venue for salvaging the project('s accuracy) against an unforseen issue? I did. I have been burnt before. A lot. 




##General overview

Using the Amelia package, we can see how much data do we truly have:

```{r missmap, echo=TRUE,show=TRUE,purl=FALSE}
flog.info("STARTED: missmap")
missmap(AnalysisAdDetailList, main = "Missing values vs observed")
flog.info("FINISHED: missmap")
```

... and the summary of the dataset:
```{r summary, echo=TRUE,show=TRUE,purl=FALSE}
dim(AnalysisAdDetailList)
summary(AnalysisAdDetailList)
```

Most variables are categorical or logical. There are three time columns: the the publish time of the ad, the calculated end time of the ad and the query time. As for numerical data, we have the commonidium charges, the independent variable, price (Ár) and the floorsize. 

Some models benefit greatly from the standardisation of numerical variables. 



##Numerical Variables

Numerical 

###Price - Ar
It is unnecessary to keep Price in such a high digit. 

```{r wrangling_Ar}
AnalysisAdDetailList$RC_ArMil<-AnalysisAdDetailList$Ar/1000000
#please add this back later on
AnalysisAdDetailList<-AnalysisAdDetailList[!is.na(AnalysisAdDetailList$RC_ArMil),]
ggplot(data = AnalysisAdDetailList) +
  geom_histogram(mapping = aes(x = RC_ArMil), binwidth = 10)

flog.info("FINISHED: wrangling_Ar")
```
Whoa can't even see these outliers
```{r wrangling_Ar2}
head(AnalysisAdDetailList[with(AnalysisAdDetailList, order(RC_ArMil),decreasing=T),'RC_Description'])
#Outliar<-AnalysisAdDetailList[with(AnalysisAdDetailList, order(RC_ArMil,decreasing = T)),]
#AnalysisAdDetailList<-AnalysisAdDetailList[AnalysisAdDetailList$RC_ArMil<200&AnalysisAdDetailList$RC_ArMil>4,]
ggplot(data = AnalysisAdDetailList[with(AnalysisAdDetailList, order(RC_ArMil,decreasing = T)),]) +
  geom_histogram(mapping = aes(x = RC_ArMil), binwidth = 10)
flog.info("FINISHED: wrangling_Ar2")
```

The response variable is negatively skewed. A logarithmic transformation might be in order. 


The principal numerical pedictive attributes  are floor size (MeretCleared), condiniumal costs, number of rooms and half rooms (rooms whose size falls below 12 sq metersor 129 square feet)  

###Floor size (MeretCleared)
I am dropping the flats whose floorsize is missing

```{r MeretClearred}
AnalysisAdDetailList<-AnalysisAdDetailList[!is.na(AnalysisAdDetailList$MeretCleared),]

AnalysisAdDetailList<-AnalysisAdDetailList[with(AnalysisAdDetailList, order(-MeretCleared)), ]
head(AnalysisAdDetailList[,c("SiteID","SiteUrl","MeretCleared")])
AnalysisAdDetailList$MeretCleared<-as.integer(as.character(AnalysisAdDetailList$MeretCleared))
```
###Number of uploaded pictures (PicNums)
```{r PicNums}
AnalysisAdDetailList$PicNums<-as.character(as.numeric(AnalysisAdDetailList$PicNums))

```


Fortunately the top 3 are development areas. For now, I am going to clip these ads off.
```{r LargeOutier}
#AnalysisAdDetailList<-AnalysisAdDetailList[AnalysisAdDetailList$MeretCleared<480,]
```
the rest f the data looks rather skewed as well a log-log transformation is now almost certainly required.


###Felszobakszama
Number of demi rooms - legally classified as such if the room's floor size is below 12 square meters.

```{r SzobaSzam}
tempvariablename="Felszobakszama"
newvariablename="RC_RoomNumDemi"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-0
AnalysisAdDetailList[AnalysisAdDetailList[newvariablename]=="5+",newvariablename]<-5

AnalysisAdDetailList[[newvariablename]]<-as.numeric(AnalysisAdDetailList[[newvariablename]])
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))


tempvariablename="SzobaSzamCleared"
newvariablename="RC_RoomNumLarge"

AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-0
AnalysisAdDetailList[AnalysisAdDetailList[newvariablename]=="5+",newvariablename]<-5
AnalysisAdDetailList[[newvariablename]]<-as.numeric(AnalysisAdDetailList[[newvariablename]])
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```



###Publish Date(PublishedTime)
The date of the  original publishing of the ad. 
```{r publishing_date}
AnalysisAdDetailList$Age<-as.Date(QueryEndTime)-as.Date(AnalysisAdDetailList$PublishedTimeCleared)
ggplot(data = AnalysisAdDetailList) +
  geom_histogram(mapping = aes(x = Age), binwidth = 10)
```



##Categorical predictors

###Query State
```{r QueryState}
knitr::kable(as.data.frame(table(AnalysisAdDetailList$QueryState)))
AnalysisAdDetailList<-AnalysisAdDetailList[AnalysisAdDetailList$QueryState=='No errors recorded',]
AnalysisAdDetailList$QueryState<-as.factor(AnalysisAdDetailList$QueryState)
```

###Real estate type(Kategoria)
The variable defined the major usage category of the real estate in question. Since the filter for apartments had been hard coded intot the webscraper query, the variable could safely be dropped:
```{r Kategoria}
knitr::kable(as.data.frame(table(AnalysisAdDetailList$Kategoria,useNA="ifany")))
```

As we can see, there is only a minimal amount of records missing due to connection errors. These were captured before a secondary loop was implemented in the crawler. Due to the low volume, we can safely drop these.

###Condition (Allapot)
```{r Allapot}
tempvariablename="Allapot"
newvariablename="RC_Condition"
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[tempvariablename]],useNA="ifany")))
templevels<-levels(AnalysisAdDetailList[[tempvariablename]])

AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"State_NotSpecified"
AnalysisAdDetailList[AnalysisAdDetailList[newvariablename]=="Új",newvariablename]<-"Új_építésű"
AnalysisAdDetailList[AnalysisAdDetailList[newvariablename]=="Új építésű",newvariablename]<-"Új_építésű"
AnalysisAdDetailList[AnalysisAdDetailList[newvariablename]=="Jó állapotú",newvariablename]<-"Jó_állapotú"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[newvariablename]])

knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```


###Alternative offer types 

####Lease rights  
```{r berletijog}
tempvariablename="Tulajdonjogberletijog"
newvariablename="RC_Title"

AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"Title_NotSpecified"
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])=="Bérleti jog",newvariablename]<-"Title_Lease"
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])=="Tulajdonjog",newvariablename]<-"Title_Ownership"
AnalysisAdDetailList[[newvariablename]]<-factor(AnalysisAdDetailList[[newvariablename]],levels=c("Title_NotSpecified","Title_Lease","Title_Ownership"),ordered=F)
#AnalysisAdDetailList<-AnalysisAdDetailList[AnalysisAdDetailList[newvariablename]=="Title_Ownership",]
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))

```

In Hungary, there is a market for leases for social housing. People granted the right to live in this monicipally owned, low rent houses may sell their leases to third parties. If properly filled, the ad contains field the factor "bérleti jog" in the tuladonjogberletijog field. Since this is a completely different market, it needs to be filtered out.

####Claims
Another latent offer type is "factoring". Claims by financial institutions, are often onloaded to the market intermediaries. These offers can be transformed into property ownership but only several months of litigation and thus, sell on discount. 

Unfortunately, jofogas have not yet implemented a flag for these ads, even though the site claims to do manual a revisions of every single offer posted. Fortunately though, the offeres are often made by whoesalers who create formulaic postings. 

####For Rent
There are plenty of people who set the for sale/ for rent flags incorrectly. After reviewing the data I  have concluded that no appartment should be rented out under 1'500'000 HUF per month.

For these latter two categores, a classification predictive type of Machine Learning problem could be formulated.
In addition to the above, it is imperative that we drop ads with missing prices data. 

I am going to drop a few variables as well:
'Helyseg' - "town" shouldn't even be in the data set, marking this is clearly either an intentional or an accidental clerical error. 
'Szobakszama' (number of rooms), 'Meret' (floor size),'Havikozoskoltseg' (monthly condo costs) PublishedTime had been parsed and recoded by the crawler so it's rather quite pointless to keep them, just as much as the  helper column 'deviance'.



###Build technology
Panel stands for 'prefab'. This building techniology was used heavily from the sixties to the late eighties and are generally considered to be low-income housing. 
```{r Building_material}
tempvariablename="Ingatlantipusa"
newvariablename="RC_Material"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"NotSpecified"

AnalysisAdDetailList[[newvariablename]]<-factor(AnalysisAdDetailList[[newvariablename]],levels=c("NotSpecified","tégla","panel","egyéb"),ordered=F)
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
AnalysisAdDetailList<-AnalysisAdDetailList[AnalysisAdDetailList[[newvariablename]]!="NotSpecified",]
```

###Heating Technology
Panel stands for 'prefab'. This building techniology was used heavily from the sixties to the late eighties and are generally considered to be low-income housing. 
```{r heating}
tempvariablename="Futestipusa"
newvariablename="RC_Heating"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"NotSpecified"
AnalysisAdDetailList[[newvariablename]]<-factor(AnalysisAdDetailList[[newvariablename]],ordered=F)
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```


Panel stands for 'prefab'. This building techniology was used heavily from the sixties to the late eighties and are generally considered to be low-income housing. 
```{r garden}
tempvariablename="Kertkapcsolatos"
newvariablename="RC_OwnYard"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"NotSpecified"

AnalysisAdDetailList[[newvariablename]]<-factor(AnalysisAdDetailList[[newvariablename]],ordered=F)
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```


### Floor -storey(emelet)

```{r Emelet}
tempvariablename="Emelet"
newvariablename="RC_FLOOR"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"NotSpecified"
#pEmeletet<-ggplot(data = AnalysisAdDetailList, mapping = aes(x = RC_FLOOR, y = RC_ArMil)) + 
#  geom_boxplot() +
#  coord_cartesian(ylim = c(0, 100))
#pEmeletet

```
Hungary uses the british type of notation system where the first floor is not the ground floor but the one immediately above it. Looking at the caht, it becomes quickly rather quite obvious that while the varianse is striking, it  is far from being linear. it  looks like that the distributions of the first, second, third and forth floors have similar characteristics.  

```{r Emeleted_dichotomised}
AnalysisAdDetailList$RC_F<-as.factor(AnalysisAdDetailList$RC_FLOOR)
tempvariablename="RC_STOREYREDUX"
AnalysisAdDetailList[AnalysisAdDetailList[[tempvariablename]] %in% c("1","2","3","4","félemelet","5","6"),tempvariablename]<-"1-6th floors"
AnalysisAdDetailList[AnalysisAdDetailList[[tempvariablename]] %in% c("7","8","9","10","10+"),tempvariablename]<-"9+"
AnalysisAdDetailList[AnalysisAdDetailList[[tempvariablename]] %in% c("földszint","magasföldszint"),tempvariablename]<-"GroundFloor"

AnalysisAdDetailList[[tempvariablename]]<-factor(AnalysisAdDetailList[[tempvariablename]],ordered=T)

#pEmeletet<-ggplot(data = AnalysisAdDetailList, mapping = aes(x = RC_STOREYREDUX, y = RC_ArMil)) + 
#  geom_boxplot() +
#  coord_cartesian(ylim = c(0, 100))
#pEmeletet

```

##Advertiser(Hirdeto)

```{r Hirdeto}
tempvariablename="Hirdeto"
newvariablename="RC_Advertiser"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
#knitr::kable(as.data.frame(table(AnalysisAdDetailList[[tempvariablename]],useNA="ifany")))
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"NotSpecified"
AnalysisAdDetailList[[newvariablename]]<-factor(AnalysisAdDetailList[[newvariablename]],ordered=F)
```


##Location Data

We have 5 variables for location data: District(Kerulet), LocStreet1,LocStreet2 and longitudinal and latitudinal data. 

District is the top localisation data, only 40 ads were missing. Manual inspection revealed that these Ads were either not flukes which were not from Budapest or missing data. 

###District (Kerulet)
```{r Kerulet}
#AnalysisAdDetailList<-AnalysisAdDetailList[!is.na(AnalysisAdDetailList$Kerulet),]
tempvariablename="Kerulet"
AnalysisAdDetailList[[tempvariablename]]<-factor(AnalysisAdDetailList[[tempvariablename]],ordered=F)
ggplot(data=AnalysisAdDetailList)+
  geom_bar(mapping = aes(x = Kerulet), fill='orange')
 facet_wrap(~ class, nrow = 2)
```


###Gmaps location:neighbourhood 

```{r neighborhood}
tempvariablename="neighborhood"
newvariablename="RC_Neighborhood"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"NotSpecified"
AnalysisAdDetailList[[newvariablename]]<-factor(AnalysisAdDetailList[[newvariablename]],ordered=F)
```



##Binary Features
These features are checked by the ad-poster and is stored in a single comma joined concatenated string on the website. THe crawer splits it up into individual booleans which then need to be converted into factors below. Due to their checkbox recording, these items can only assume the values of yes/no without missing values. I am going to a wording that makes it immediately obvious that the negative value doesn't necessarily mean explicit false and 'lazily filled forms' will show up here. 

Due to them being binary, there is not a lot of prepocessing could possibly do.

###Built-in Kitchen(Konyha)
```{r Kitchen}
tempvariablename="konyha"
newvariablename="RC_KITCHEN_FURN"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])==TRUE,newvariablename]<-"KITCH_FURN"
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])==FALSE,newvariablename]<-"KITCH_NOT_FURN"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[newvariablename]])
AnalysisAdDetailList[[tempvariablename]]<-NULL

knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```

###Parking (Parkolas)
```{r Parkingspace}
tempvariablename="Parkolas"
newvariablename="RC_PARKING"
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[tempvariablename]],useNA="ifany")))
templevels<-levels(AnalysisAdDetailList[[tempvariablename]])
AnalysisAdDetailList[is.na(AnalysisAdDetailList[tempvariablename]),tempvariablename]<-"NotSpecified"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[tempvariablename]])
AnalysisAdDetailList[[tempvariablename]]<-NULL

knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```

### Accessability(Akadalymentesitett)
```{r Accessability}
tempvariablename="Akadalymentesitett"
newvariablename="RC_Accessability"

AnalysisAdDetailList[(AnalysisAdDetailList[tempvariablename])==TRUE,tempvariablename]<-"Handycapready"
AnalysisAdDetailList[(AnalysisAdDetailList[tempvariablename])==FALSE,tempvariablename]<-"NothandyCap"

AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[tempvariablename]])
AnalysisAdDetailList[[tempvariablename]]<-NULL
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```


###Appliances (Gepesitett)
Indicates whether household appliances (washing machine, fridge) are included in the offer. 
```{r Appliances}
tempvariablename="Gepesitett"
newvariablename="RC_Appliances"
AnalysisAdDetailList[(AnalysisAdDetailList[tempvariablename])==TRUE,tempvariablename]<-"Appliances_included"
AnalysisAdDetailList[(AnalysisAdDetailList[tempvariablename])==FALSE,tempvariablename]<-"Appliances_included_NOT"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[tempvariablename]])


knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```

###Alarm (Riaszto)
```{r Alarm}
tempvariablename="Riaszto"
newvariablename="RC_Alarmed"
AnalysisAdDetailList[(AnalysisAdDetailList[tempvariablename])==TRUE,tempvariablename]<-"Alarmed"
AnalysisAdDetailList[(AnalysisAdDetailList[tempvariablename])==FALSE,tempvariablename]<-"Alarmed_NOT"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[tempvariablename]])
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```

###Elevator(Lift)

```{r Lift}
tempvariablename="Lift"
newvariablename="RC_Elevator"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[tempvariablename]])

knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```

###Stand-alone Toilet (KulonWC)
Indicates whether the toilet is located within the bathroom or in a separate room. 
```{r Separate_Toilet}
tempvariablename="KulonWC"
newvariablename="RC_Separate_Toilet"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])==TRUE,newvariablename]<-"Toiler_Separate"
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])==FALSE,newvariablename]<-"Toiler_Separate_NOT"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[newvariablename]])

knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```

###Air Conditioning (Legkondicinalo)
```{r AirConditioning}
tempvariablename="Legkondicinalo"
newvariablename="RC_AC"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])==TRUE,newvariablename]<-"AC"
AnalysisAdDetailList[(AnalysisAdDetailList[newvariablename])==FALSE,newvariablename]<-"AC_Not"
AnalysisAdDetailList[[newvariablename]]<-as.factor(AnalysisAdDetailList[[newvariablename]])

knitr::kable(as.data.frame(table(AnalysisAdDetailList[[tempvariablename]],useNA="ifany")))
```

###Ceiling Height
"room or ceiling height" is basically a proxy for pre- turn-of-the-century (to the 20th I know. I know, it is scary.) or later. The building is considered uneconomical but prestigeous to be build in the style of "large ceiling heights". 

```{r Belmagassag}
tempvariablename="Belmagassag"
newvariablename="RC_Room_Height"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
AnalysisAdDetailList[is.na(AnalysisAdDetailList[newvariablename]),newvariablename]<-"Height_NotSpecified"
AnalysisAdDetailList[[newvariablename]]<-factor(AnalysisAdDetailList[[newvariablename]],levels=c("Height_NotSpecified","3 m alatt","3 m fölött"),ordered=F)
knitr::kable(as.data.frame(table(AnalysisAdDetailList[[newvariablename]],useNA="ifany")))
```



##Description(Description)
Description is a freetext field. It is a prime target for text analysis. At a later time. 
```{r Description}
tempvariablename="Description"
newvariablename="RC_Description"
AnalysisAdDetailList[[newvariablename]]<-AnalysisAdDetailList[[tempvariablename]]
#reference numbers
pattern<-".*azonosító:[[:space:]]*([^[:space:]]*)[[:space:]]*"
pattern2<-".*referencia szám[[:space:]]*:[[:space:]]*([^[:space:]]*)[[:space:]]*"
patternM<-".*(M[[:digit:]]{6,6})[^[:digit:]].*"

totalpattern<-paste0(pattern2,"|",pattern2,"|",patternM)
#AnalysisAdDetailList$RefNum<-gsub((pattern)|(pattern2)|(patternM),"\\1",AnalysisAdDetailList$RC_Description,ignore.case=T)
AnalysisAdDetailList$ReferenceNumber<-ifelse(grepl(totalpattern,AnalysisAdDetailList$RC_Description,ignore.case=T),gsub(totalpattern,"\\1\\2\\3",AnalysisAdDetailList$RC_Description,ignore.case=T),NA)

#AnalysisAdDetailList$RefNum<-ifelse(grepl(pattern,AnalysisAdDetailList$RC_Description,ignore.case=T),sub(pattern,"\\1",AnalysisAdDetailList$RC_Description,ignore.case=T),NA)

#AnalysisAdDetailList$RefNum2<-ifelse(grepl(pattern2,AnalysisAdDetailList$RC_Description,ignore.case=T),gsub(pattern2,"\\1",AnalysisAdDetailList$RC_Description,ignore.case=T),NA)

AnalysisAdDetailList$RefNumM<-ifelse(grepl(patternM,AnalysisAdDetailList$RC_Description,ignore.case=T),gsub(patternM,"\\1",AnalysisAdDetailList$RC_Description,ignore.case=T),NA)
```

##Rescoping
```{r rescoping}
flog.info("STARTED: rescoping")
AnalysisAdDetailList$Deviance<-ifelse(QueryEndTime-AnalysisAdDetailList$QueryTime>0,QueryEndTime-AnalysisAdDetailList$QueryTime,9999999999999)
n_occur<-data.frame(table(AnalysisAdDetailList$SiteID))
if(exists("n_occur")){rm(n_occur)}
Scope<-"Total"
if(Scope=="Total") {
AnalysisAdDetailList<-AnalysisAdDetailList[with(AnalysisAdDetailList, order(QueryTime,decreasing = T)),]
}
if(Scope=="Snapshot") {
AnalysisAdDetailList<-AnalysisAdDetailList[with(AnalysisAdDetailList, order(Deviance)),]
}

flog.info("FINISHED: identifying_the_proper_IDs")
```

##Dropping Unused columns
```{r dropping_columns}
flog.info("STARTED: Droping unused columns")
AnalysisAdDetailList<-AnalysisAdDetailList[,!(names(AnalysisAdDetailList) %in% c("Helyseg","Meret","Ar","Deviance","Szobakszama", "Kategoria","PublishedTime","QueryState", "Havikozoskoltseg","Ingatlantipusa","Lift","Gepesitett","SzobaSzamCleared","Felszobakszama","SiteUrl","Riaszto","KulonWC","Tulajdonjogberletijog","Legkondicinalo","Belmagassag","Futestipusa","Allapot","Emelet","Butorozotte","Description","Ingatlanfelszereltsege","Hirdeto","Kertkapcsolatos","Latitude","Longitude"))]
flog.info("FINISHED: dropping_columns")
```

##Exporting to the ShinyApp
```{r Exporting_to_Shiny}
flog.info("STARTED: Exporting_to_Shiny")
#I had to push the filters and calculations here in order to fit into the RAM limits of the free tier of shinapps.io. 
ShinySessionTableFile<-"ShinySessionTable.Rda" #the file name for exportedSessionTable
ShinyAdListFile<-"ShinyAdlistTable.Rda" #the file name for exportedAdList 
ShinyDetailTableFile<-"ShinyDetailTable.Rda" #the file name for exportedShiny

Shinyfolder<-paste(getwd(),"ShinyRealEstimator",sep="/") # the location of the Shiny App

#Pre-processing the sessions' table ----

exportedSessionTable<-AnalysisSessionTable[AnalysisSessionTable$ProjectName=="JofogasLakasokBPFull" & AnalysisSessionTable$ResultNumber>12000 & AnalysisSessionTable$ReceivedAds>12000 & !is.na(AnalysisSessionTable$StartTime) & AnalysisSessionTable$Status=="Completed",c("SessionID","StartTime","ResultNumber","ReceivedAds","NewAdNumber","ClosedAdNumber")]

exportedSessionTable<-exportedSessionTable[order(exportedSessionTable$StartTime, decreasing = TRUE),]

exportedSessionTable$StartTimeStarter<-as.POSIXct(ifelse(hour(exportedSessionTable$StartTime)<11,ceiling_date(exportedSessionTable$StartTime-days(1),unit = "day",change_on_boundary=FALSE)-seconds(1) ,exportedSessionTable$StartTime),origin="1970-01-01",tz="Europe/Budapest")
exportedSessionTable$SnapshotDate<-as.Date(exportedSessionTable$StartTimeStarter,tz="Europe/Budapest")
exportedSessionTable$StartTimeStarter<-NULL
exportedSessionTable$MissRatePercent<-round(((exportedSessionTable$ResultNumber-exportedSessionTable$ReceivedAds)/exportedSessionTable$ResultNumber)*100,2)
exportedSessionTable<-exportedSessionTable[!duplicated(exportedSessionTable$SnapshotDate),] #multiple- single day Sessions
#Preprocessing the AdList table ----

exportedAdList<-AnalysisAdList[(AnalysisAdList$SessionID %in% exportedSessionTable$SessionID),] # dropping unreported Ads
exportedAdList<-exportedAdList[,c("SiteID","SessionID","QueryTime","titles")]
exportedAdList<-merge(x=exportedAdList,y=exportedSessionTable[,c("SessionID","StartTime","SnapshotDate")],by=c("SessionID"),all.x = TRUE,all.y=FALSE)

exportedcolumns<-c("RC_ArMil","SiteID","QueryTime","RC_Title","Kerulet","MeretCleared","RC_Condition","RC_Heating","RC_FLOOR","RC_Material","RC_PARKING")
#Preprocessing the AdDetails table ----
exportedShiny<-AnalysisAdDetailList[,(names(AnalysisAdDetailList) %in% exportedcolumns)]
exportedAdList<-merge(x=exportedAdList,y=exportedShiny,by=c("SiteID","QueryTime"),all.x = TRUE,all.y=FALSE)

saveRDS(exportedSessionTable,paste(Shinyfolder,ShinySessionTableFile,sep="/"))
saveRDS(exportedAdList,paste(Shinyfolder,ShinyAdListFile,sep="/"))
#Unnecessary, candidate for deletion saveRDS(exportedShiny,paste(Shinyfolder,ShinyDetailTableFile,sep="/")) 


rm(AnalysisAdList)
rm(exportedShiny)
rm(AnalysisSessionTable)
#Uploading to drop-box ----
library(rdrop2)
token <- readRDS("droptoken.rds")

drop_upload(paste(Shinyfolder,ShinySessionTableFile,sep="/"), path = NULL, mode = "overwrite", autorename = TRUE,  mute = FALSE, verbose = FALSE, dtoken = token)

drop_upload(paste(Shinyfolder,ShinyAdListFile,sep="/"), path = NULL, mode = "overwrite", autorename = TRUE,  mute = FALSE, verbose = FALSE, dtoken = token) 

#Unnecessary, candidate for deletion drop_upload(paste(Shinyfolder,ShinyDetailTableFile,sep="/"), path = NULL, mode = "overwrite", autorename = TRUE,  mute = FALSE, verbose = FALSE, dtoken = token) 
flog.info("FINISHED: Exporting_to_Shiny")
```

##Fuzzy Deduplication
After finishing the data cleaning, we are now able to use these variables to find fuzzyduplicates, or near matches using compare.linkage and recordlinkage() packages. The following experimenting relies on information gathered from the article by the package authors( available here:https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf)

There are two broad approaches for identifying duplicates:stochastic methods and methods relying on machine learning. The former of which relies on finding the parameter weight threshold where the likeklihood of 
a successful classification is maximum. 

Let's load the library and generate a simple control table for comparison.
```{r RecordLinkage,purl=FALSE, eval=FALSE}
flog.info("STARTED: RecordLinkage")
library(RecordLinkage)
#small reproducible test
DedupeTestSet<-AnalysisAdDetailList[AnalysisAdDetailList$RC_RoomNumLarge=="3" & AnalysisAdDetailList$Kerulet=="XVIII." & AnalysisAdDetailList$MeretCleared==74,]

#large TestSet
DedupeTestSet<-AnalysisAdDetailList[AnalysisAdDetailList$Kerulet=="XVIII.",]

  CompareBy<-c("SiteID","RC_Description","MeretCleared","Kerulet","EmeletOrg","RC_ArMil","PublishedTimeCleared")
  Blockfields<-c("Kerulet","MeretCleared","EmeletOrg")
  ExcludeTheseFields<-c("SiteID","PublishedTimeCleared")
DedupeTestLimited<-DedupeTestSet[,CompareBy]
#DedupeFullLimited<-AnalysisAdDetailList[,CompareBy]

#calculating the number of comparison pairs
Estimation <- AnalysisAdDetailList %>%
  group_by_at(vars(one_of(Blockfields))) %>%
  summarize(n = n(),npairs=n*(n-1)/2)

sum(Estimation$npairs)

#back to business

#adding fuzzy comparison, the numeric variables seem to be working out well.
#rpairs <- compare.dedup(DedupeTestShort, blockfld = c("Kerulet","MeretCleared"))
 #the default fuzzyrpairs is jarowinkler, 

#the strcmp doesn't have to fuzzycompare anything. column position supplied.
fuzzyrpairsDefault<-compare.dedup(DedupeTestLimited, blockfld = Blockfields, strcmp = which(colnames(DedupeTestLimited) %in% c("RC_Description")), exclude=which(colnames(DedupeTestLimited) %in% ExcludeTheseFields))

#the alternative is levenshteinSim
#fuzzyrpairslevenshteinSim<-compare.dedup(DedupeTestLimited, blockfld = Blockfields, strcmp=TRUE, strcmpfun =  levenshteinSim)

#the the stochastic method ( ans underlying stochastic modell, use manual fixing)
fuzzyrpairsDefault$pairs

emWeightedPairs<-emWeights(fuzzyrpairsDefault)

#With EPI. We don't do EPI
#epiWeightedPairs<-epiWeights(fuzzyrpairsDefault, e = 0.05)
#epiresult <- epiClassify(fuzzyrpairsDefault, 0.55)

#Eith EM
emResult <- emClassify(emWeightedPairs)
summary(emResult)
#determine the weight of cutoff for matches and non matches using getParetoThreshold. The overlapping section is continuous and linear

Adlinks<-getPairs(emResult,show="links", single.rows=TRUE)
#Show the matching records
AllPairs<-getPairs(emResult, single.rows=TRUE)

#The records are listed in the getPairs function returned us the matches in an Rj1:Rj2 format where Rj1 and Rj2 are two records.
#we need to group multiple records together. TThe simpler way is just replacing items until the vectors are normalised.
#The second option borrows the ideas from network studies. 'Unmelting', or using dplyr's terminology (gathering)(check it!) the Aj:Rj2 nx2 data frames into a matrix where the values store the status or value of the link between the row's  and the column's agent, we will get a table that is referred to the 'adjacency matrix'. The adjacency matrices have a very interesting attribute: While it is identical form shows the one step connections bertween two items, its square ( A * A ) gives us the 2 step distance, it's cube the 3 step distance and so on. ASsuming that we define a cutoff distance of 2 steps, then summng and normaling them gives us whether connection exists between two items: sign(A + A*A) 

#Matrixing 
test<-dcast(AllPairs, formula = SiteID.1 ~ SiteID.2, value="Weight", fill=0)
test_snapshot<-test

if(!is.null(test$SiteID.1))
{
 row.names(test)<-test$SiteID.1
 test$SiteID.1<-NULL
}
#the two step relations require the adjacency matrix to be squared. This requires a) matrices and b) need the weights need to be normalised onto to a [0-1] scale vbecasue squaring these values turn negative values into positives which is classified as links.. 
testMatrix<-as.matrix(test) #matrix multiplication requires matrices
testSignified<-sign(testMatrix>0) #collaping it to 0,1
testSignified[!upper.tri(testSignified)] <- 0 #halving
testsq<-testSignified %*% testSignified
TestSUMM<-testSignified+testsq
TestSUMM<-sign(TestSUMM>0)
TestSUMM[1:30,1:30]

Adlinks$PublishedTimeCleared.3<-ifelse(Adlinks$PublishedTimeCleared.1<=Adlinks$PublishedTimeCleared.2,Adlinks$PublishedTimeCleared.1, Adlinks$PublishedTimeCleared.2)
Adlinks$PublishedTimeCleared.4<-ifelse(Adlinks$PublishedTimeCleared.1<=Adlinks$PublishedTimeCleared.2,Adlinks$PublishedTimeCleared.2, Adlinks$PublishedTimeCleared.1)

Adlinks$SiteID.3<-ifelse(Adlinks$PublishedTimeCleared.1<=Adlinks$PublishedTimeCleared.2,Adlinks$SiteID.1, Adlinks$SiteID.2)
Adlinks$SiteID.4<-ifelse(Adlinks$PublishedTimeCleared.1<=Adlinks$PublishedTimeCleared.2,Adlinks$SiteID.2, Adlinks$SiteID.1)

Adlinks$RC_Description.3<-ifelse(Adlinks$PublishedTimeCleared.1<=Adlinks$PublishedTimeCleared.2,Adlinks$RC_Description.1, Adlinks$RC_Description.2)
Adlinks$RC_Description.4<-ifelse(Adlinks$PublishedTimeCleared.1<=Adlinks$PublishedTimeCleared.2,Adlinks$RC_Description.2, Adlinks$RC_Description.1)

Adlinks$SiteID.1<-Adlinks$SiteID.3
Adlinks$SiteID.2<-Adlinks$SiteID.4
Adlinks$RC_Description.1<-Adlinks$RC_Description.3
Adlinks$RC_Description.2<-Adlinks$RC_Description.4
Adlinks$PublishedTimeCleared.1<-Adlinks$PublishedTimeCleared.3
Adlinks$PublishedTimeCleared.2<-Adlinks$PublishedTimeCleared.4

Adlinks$RC_Description.3<-NULL
Adlinks$RC_Description.4<-NULL

Adlinks$SiteID.3<-NULL
Adlinks$SiteID.4<-NULL

Adlinks$PublishedTimeCleared.3<-NULL
Adlinks$PublishedTimeCleared.4<-NULL

Adlinks <- Adlinks[order(xtfrm(Adlinks$PublishedTimeCleared.1), xtfrm(Adlinks$PublishedTimeCleared.2)),, drop = FALSE]

if(exists("FinalAdLinks")){
  FinalAdLinks<-Adlinks[1,]
} else {
  #FinalAdLinks<-rbind()
}

#replace records
AdPairs$SiteID.1<-replace(AdPairs$SiteID.1, AdPairs$SiteID.1==AdPairs[index,"SiteID.2"], AdPairs[index,"SiteID.1"])
AdPairs$SiteID.1<-replace(AdPairs$SiteID.1, AdPairs$SiteID.1==AdPairs[index,"SiteID.2"], AdPairs[index,"SiteID.1"])
AdPairs$SiteID.1<-replace(AdPairs$SiteID.1, AdPairs$SiteID.1==AdPairs[index,"SiteID.2"], AdPairs[index,"SiteID.1"])
#remove duplicates


write.csv(Adlinks,file="pairs.csv")
AdPairs<-Adlinks[,c("SiteID.1","SiteID.2")]

# consolidating the table 

#for(index in 1:nrow(AdPairs)){
#  AdPairs$SiteID.1<-replace(AdPairs$SiteID.1, AdPairs$SiteID.1==AdPairs[index,"SiteID.2"], AdPairs[index,"SiteID.1"])
#  Adpairs[index,"SiteID.2"]
#}

write.csv(AdPairs,file="correctedpairs.csv")
#redoing this on entire dataset
#DedupeTestLong<-AnalysisAdDetailList[,CompareBy]
#LongfuzzyrpairsDefault<-compare.dedup(DedupeTestLong, blockfld =c("Kerulet","MeretCleared"), strcmp = TRUE, )
flog.info("FINISHED: RecordLinkage")
```
#Feature Engineering
I am going to ditch unspecified sale types then do a skewedness analysis




```{r Feature_engineering}
library(psych)
AnalysisAdDetailList2<-AnalysisAdDetailList[AnalysisAdDetailList$RC_Title=="Title_Ownership",]
AnalysisAdDetailList2<-AnalysisAdDetailList2[AnalysisAdDetailList2$RC_ArMil>4,]
skew(AnalysisAdDetailList2$RC_ArMil)
qqnorm(AnalysisAdDetailList2$RC_ArMil)
qqline(AnalysisAdDetailList2$RC_ArMil)
AnalysisAdDetailList2$RC_ArMil <- log(AnalysisAdDetailList2$RC_ArMil)
skew(AnalysisAdDetailList2$RC_ArMil)
qqnorm(AnalysisAdDetailList2$RC_ArMil)
qqline(AnalysisAdDetailList2$RC_ArMil)
ggplot(data= AnalysisAdDetailList2, aes(x=RC_ArMil)) +
        geom_histogram() + labs(x='Price Historgram')


```

#Data Splitting and Cross-validation

CrossValidation is a measure that is  designed to harden the model from the error of overfitting. Overfitting happens when the number of independent variables in the model is so high, that the model begins to predict not the attributes of the population but the sample itself, it starts to show bias. In practice this means that as the number of independent variables increases, the error of the models for the elements of the sample continues to diminish, but the level of error against new observations ulation assumes a U - parabola - shape and begins to increase as lim ->x(i)max. 

Unless the samplesize is prohibitingly small, the sample needs to be split into training and testing subsamples. This, however introduces variance to the equation. Variance, bias computational need and expenses are the factors of the optimisation task of finding the appropriate cross validation technique.

Bias is the bad efficiency of the model.Variance is, however, the ariance of predictive value  from dataset to dataset. Generally speaking, simpler models have more bias but less variance and vica versa.

It is generally true that more complex models can have very high vari-
ance, which leads to over-fitting. On the other hand, simple models tend not
to over-fit, but under-fit if they are not flexible enough to model the true
relationship (thus high bias). Also, highly correlated predictors can lead to
collinearity issues and this can greatly increase the model varianc


It is a two stage process corresponding to the phases of data analysis. 
1.The first split determines the Test sample upon which the final power of the model will be tested

use the createDataPartition function from caret for stratified random splits. For simple random splits base R's sample might sufice.

1. The second split of the training set will attempt to mitigate variance and bias using resampling.

Splitting the original sample into the test and training can be done using several mathematical methos. The dissimilarity method relies on grabbing the most different obs. after the initial osbervations were selected.

Stratified samples keep the response variable's distribution equal in the test and train subsetted populations. 

Hold-out cross validation is basically simple data spitting with no resampling. 

The k-fold cross validation technique take the sample, cuts into 3-5-10 pieces and runs k number of model fittings with the 1 subsample serving as test sample each. 

k-fold has a relatively high variance value. When using smaller k values, it's variance is matched with the bootstrap method

the leave-one-out-hold out crossvalidation is the extreme version of the k-fold CV where k = n.


Bootstrap method selects sample items by allowing observations to be selected multiple times. Some obs. will show up multiple times, some, never. Their variance is relatively small but the bias ould be enormous. 


The various crossvalidation techniques combat this by splitting a sample into training and testing subsets. This, ofcourse, can additionally introduce sampling errors.

One of the most widely  used techinque is the so called K-fold crossvalidation, which is essentially test on one, train on K-1 type of configuration with the typical K values of 3,5 and 10.

##Crude deduplication

```{r crude_deduplication,purl=FALSE}
AnalysisAdDetailList<-AnalysisAdDetailList[!duplicated(AnalysisAdDetailList$SiteID),]
AnalysisAdDetailList<-AnalysisAdDetailList[!duplicated(AnalysisAdDetailList[c("MeretCleared","RC_Description","Kerulet")]),]
```

##Data Partitioning
```{r data_partitioning,purl=FALSE}
#ModelData<-AnalysisAdDetailList[AnalysisAdDetailList$Kerulet  %in% c("VIII.","IX.") &!(is.na(AnalysisAdDetailList$RC_ArMil)|is.na(AnalysisAdDetailList$MeretCleared)|is.na(AnalysisAdDetailList$Location1)),]


#removing the outliars and missing values
ModelData<-AnalysisAdDetailList[!(is.na(AnalysisAdDetailList$RC_ArMil)|is.na(AnalysisAdDetailList$MeretCleared)|is.na(AnalysisAdDetailList$Kerulet)),]
ModelData<-ModelData[ModelData$MeretCleared<480,]

ModelData$Location1<-as.factor(ModelData$Location1)

dim(ModelData)

TrainingIndex <- createDataPartition(ModelData$RC_ArMil, p = .80, list = FALSE,groups = min(5, length(ModelData$RC_ArMil)))
TrainingSet <- ModelData[ TrainingIndex,]
TestSet <- ModelData[-TrainingIndex,]
dim(TrainingSet)
dim(TestSet)
flog.info("FINISHED: data_partitioning")
```

##Cross_Validation
```{r cross_validation,purl=FALSE}
TRC_CV10<-trainControl(method="CV", number=10)
TRC_CV5<-trainControl(method="CV", number=5)
flog.info("FINISHED: cross_validation")
```

#Model selection

1.Models have different attributes. accuracy, specificity, computational costs flexibility. 
The general principle is to start with a robust, highly powerful but opaque model like support vectors and random forests to find the practical maximum for the models

1. Find one with less variables and efficiency within one standard deviation of the above defined maximum.

For example,
if a model is created to predict two classes, sensitivity and specificity may
be used to characterize the efficacy of models (see Chap. 11). If the data
set includes more events than nonevents, the sensitivity can be estimated
with greater precision than the specificity. With increased precision, there is
a higher likelihood that models can be differentiated in terms of sensitivity
than for specificity.

###Measuremenets:

    Sensitivity (also called the true positive rate, the recall, or probability of detection[1] in some fields) measures the proportion of positives that are correctly identified as such (e.g. the percentage of sick people who are correctly identified as having the condition).
    Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition).

#Bivariate visualisations
Bivariate plot between every pair of attributes

```{r bivariate heatmap,purl=FALSE,eval=FALSE}
#Template used:http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

HeatMapTable <- TrainingSet[, c("RC_ArMil","MeretCleared","KozkoltsegFtho","RC_RoomNumLarge")]
CorMat<-round(cor(HeatMapTable,use = "pairwise.complete"),2)
head(CorMat)

get_lower_tri<-function(CorMat){
    CorMat[upper.tri(CorMat)] <- NA
    return(CorMat)
  }
  get_upper_tri <- function(CorMat){
    CorMat[lower.tri(CorMat)]<- NA
    return(CorMat)
  }
  reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
  }

 # Reorder the correlation matrix
CorMat <- reorder_cormat(CorMat)
upper_tri <- get_upper_tri(CorMat)
melted_cormat <- melt(upper_tri, na.rm=TRUE)
# Create a heatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

ggheatmap+
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

pKitchen<-ggplot(data = TrainingSet, mapping = aes(x = RC_KITCHEN_FURN, y = RC_ArMil)) +
geom_boxplot() +
coord_cartesian(ylim = c(0, 100))

pAppliances<-ggplot(data = TrainingSet, mapping = aes(x = RC_Appliances, y = RC_ArMil)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 100))
pAlarm<-ggplot(data = TrainingSet, mapping = aes(x = RC_Alarmed, y = RC_ArMil)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 100))
pWC<-ggplot(data = TrainingSet, mapping = aes(x = RC_Separate_Toilet, y = RC_ArMil)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 100))
pAC<-ggplot(data = TrainingSet, mapping = aes(x = RC_AC, y = RC_ArMil)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 100))
pCT<-ggplot(data = TrainingSet, mapping = aes(x = RC_Title, y = RC_ArMil)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 100))

#pEmeletet<-ggplot(data = TrainingSet, mapping = aes(x = RC_STOREYREDUX, y = RC_ArMil)) + 
#  geom_boxplot() +
#  coord_cartesian(ylim = c(0, 100))



pAccess<-ggplot(data = TrainingSet, mapping = aes(x = RC_Accessability, y = RC_ArMil)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 100))

grid.arrange(pKitchen, pAppliances, pAlarm,pWC,pAC,pAccess,pCT, ncol=3)
```

```{r visualisation,purl=FALSE,eval=FALSE}
p5<-ggplot(data=TrainingSet)+
  geom_bar(mapping = aes(x = Kerulet), fill='orange')

p4<-ggplot(data=TrainingSet)+
  geom_bar(mapping = aes(x = Kerulet, fill=RC_Material), position="dodge")


p3<-ggplot(data=TrainingSet)+
  geom_point(mapping = aes(x=MeretCleared, y=RC_ArMil), position="jitter")+
  geom_smooth(mapping = aes(x = MeretCleared, y = RC_ArMil, group = Kerulet))


p1<-ggplot(data=TrainingSet)+coord_cartesian(xlim = c(0, 150), ylim = c(0, 70))+
  geom_point(mapping = aes(x=MeretCleared, y=RC_ArMil, colour=Kerulet), position="jitter")+
  geom_smooth(mapping = aes(x = MeretCleared, y = RC_ArMil, group = Kerulet))

KeruletSummery<-TrainingSet%>%
  group_by(Kerulet)%>%
  summarise(
  RC_ArMil=mean(RC_ArMil, na.rm=TRUE),
  n=n()
  )
p2<-ggplot(KeruletSummery, aes(Kerulet,RC_ArMil)) + 
  geom_point()

grid.arrange(p1, p4, p3,p2,p5, ncol=2)
```

#Linear regressions 
```{r DummyVars_LinearRegression,purl=FALSE}
flog.info("STARTED: DummyVars_LinearRegression")

#KeruletOnly
TrainingSet$MC2<-TrainingSet$MeretCleared^2
TrainingSet$MeretLog<-log(TrainingSet$MeretCleared)
TrainingSet$LogMil<-log(TrainingSet$RC_ArMil)
TrainingSet$MeretExp<-exp(TrainingSet$MeretCleared)

#Limiting the dataset to the variables available to the exported version:

XFiltered<-TrainingSet[,colnames(TrainingSet) %in% exportedcolumns & !colnames(TrainingSet) %in% c("SiteID","QueryTime","RC_ArMil")]
#simple interactive linear regression using the well known FT/sqm measure. It requires an interactive effect which is added using a colon

RC_ArMil<-TrainingSet$RC_ArMil
QueryTime<-TrainingSet$QueryTime

LM_Basic_Dummies <- dummyVars(~Kerulet+MeretCleared+Kerulet:MeretCleared,
                       data = XFiltered,
                       levelsOnly = TRUE)

LM_Interact_Basic <- data.frame(XFiltered,predict(LM_Basic_Dummies,XFiltered)) #adding the dummies
LM_Interact_Basic <- LM_Interact_Basic[,!names(LM_Interact_Basic) %in% c("MeretCleared","Kerulet")] #Removing the precursors 

LM_modelbasic <- train(x = LM_Interact_Basic, y = RC_ArMil,
  method = "lm",
  metric="RMSE",
  trControl = TRC_CV10,
  na.action="na.exclude")

saveRDS(LM_modelbasic, file="Models_LM_BasicModel.rda")
testResults <- data.frame(SiteID=TrainingSet$SiteID,QueryTime = TrainingSet$QueryTime,XFiltered,RC_ArMil = RC_ArMil, Linear_Regression = predict(LM_modelbasic, LM_Interact_Basic))


testResults$ErrorVal<-testResults$RC_ArMil-testResults$Linear_Regression
LM_modelbasic
#interaction only RMSE:15




#negyzetes taggal
LM_modelquad<-train(RC_ArMil~MeretCleared+MC2+Kerulet,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_modelquad)

#negyzetes tagnelkul tisztan
# RMSE      Rsquared   MAE     
#  13.19065  0.7115969  8.563517
LM_modelsimple<-train(RC_ArMil~MeretCleared+Kerulet,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_modelsimple)

LM_modelquadInteraction<-train(RC_ArMil~MeretCleared:Kerulet+MC2:Kerulet,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_modelquadInteraction)


#withLOG

#RMSE      Rsquared   MAE     
#12.27934  0.7496871  7.691969

LM_modelquadInteractionwithLog<-train(LogMil~MeretLog:Kerulet+MC2:Kerulet+MeretCleared:Kerulet+RC_Condition+RC_PARKING,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_modelquadInteractionwithLog)

LM_modelquadInteractionwithexp<-train(RC_ArMil~MeretExp:Kerulet+MeretLog:Kerulet+MC2:Kerulet+MeretCleared:Kerulet+RC_Condition+RC_PARKING,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_modelquadInteractionwithexp)
flog.info("FINISHED: DummyVars_BasicModels")
```

#Random Forest
```{r DummyVars_RandomForest,purl=FALSE}
mtryGrid <- data.frame(mtry = floor(seq(1, ncol(XFiltered), length = 1)))

 rfTune <- train(x = XFiltered, y = RC_ArMil,
                 method = "rf",
                 tuneGrid = mtryGrid,
                 ntree = 200,
                 importance = TRUE,
                 trControl = TRC_CV10)


XYFiltered<-XFiltered
XYFiltered$RC_ArMil<-RC_ArMil 
 
rfMode_Basic<-randomForest(RC_ArMil~MeretCleared+Kerulet,data=XYFiltered,ntrees = 500)
print(rfMode_Basic$FinalModel)


rf_model <- randomForest(RC_Condition ~ Kerulet+RC_ArMil,data = XYFiltered)
plot(rf_model, ylim=c(0,0.36))
legend('topright', colnames(rf_model$err.rate), col=1:8, fill=1:8)



rfMode_Basic<-randomForest(RC_ArMil~MeretCleared+Kerulet,data=XFiltered,ntrees = 1000)
print(rfMode_Basic$FinalModel)

rfModel2<-randomForest(RC_ArMil~MeretCleared+Kerulet,data=XFiltered,ntrees = 1000)

```
The caret package provides an excellent standardised form for administering multiple Machine Learning models. 

```{r caret_LM_model_fitting,purl=FALSE}
if(LocationData)
{
  #LM_model1  
#RMSE      Rsquared   MAE     
#12.41814  0.7445562  7.747817  
LM_model1<-train(RC_ArMil~MeretCleared*Kerulet,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_model1)
  
LM_model9<-train(RC_ArMil~MeretCleared+MeretCleared^2+RC_PARKING+RC_Room_Height+RC_RoomNumDemi+RC_RoomNumLarge+Kerulet+RC_Condition+RC_Material+Futestipusa+RC_STOREYREDUX,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_model9)
  
  
LM_model7<-train(RC_ArMil~MeretCleared+RC_Title+RC_PARKING+RC_Room_Height+RC_RoomNumDemi+RC_RoomNumLarge+Location1+Kerulet+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_model7)

#PredSet<-TrainingSet[,c("MeretCleared","RC_PARKING","RC_Room_Height","RC_RoomNumDemi","RC_RoomNumLarge","Location1","Kerulet","RC_Condition","RC_Material","RC_Heating","RC_STOREYREDUX")]

PredSet<-TrainingSet[,c("MeretCleared","RC_PARKING","RC_Room_Height","RC_RoomNumDemi","RC_RoomNumLarge","Kerulet","RC_Condition","RC_Material","RC_Heating","RC_STOREYREDUX")]

LM_model8<-train(RC_ArMil~MeretCleared+RC_PARKING+RC_Room_Height+RC_RoomNumDemi+RC_RoomNumLarge+Location1+Kerulet+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX+RC_KITCHEN_FURN+RC_Appliances,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_model8)

LM_nonform_1<- train(PredSet, TrainingSet$RC_ArMil, method = "lm",  preProc = c("center", "scale"), trControl = trainControl(method = "cv"),na.action="na.exclude")




}

#svmRTuned <- train(PredSet, TrainingSet$RC_ArMil, method = "svmRadial",  preProc = c("center", "scale"),  tuneLength = 14, trControl = trainControl(method = "cv"))

#svm2<-train(RC_ArMil~MeretCleared+RC_Title+Kerulet+Location1, data=TrainingSet, method = "svmRadial", tuneLength = 14, trControl = trainControl(method = "cv"))

#svm2

if(ZipData & DOsvm){
svmNeighbour<-train(RC_ArMil~MeretCleared, data=TrainingSet, method = "svmRadial", tuneLength = 14, trControl = trainControl(method = "cv"))
svmNeighbour
}
ZipData<-FALSE
if(ZipData){
LM_Zipmodel1<-train(RC_ArMil~MeretCleared+MeretCleared*RC_Neighborhood+RC_Title+RC_PARKING+RC_Room_Height+RC_RoomNumDemi+RC_RoomNumLarge+RC_Neighborhood+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX+RC_KITCHEN_FURN+RC_Appliances,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_Zipmodel1)

LM_Kermodel0<-train(RC_ArMil~MeretCleared+MeretCleared^2+MeretCleared*Kerulet+RC_Title+RC_PARKING+RC_Room_Height+RC_RoomNumDemi+RC_RoomNumLarge+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX+RC_KITCHEN_FURN+RC_Appliances+RC_PARKING,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_Kermodel)


LM_Kermodel1<-train(RC_ArMil~MeretCleared+MeretCleared^2+MeretCleared*Kerulet+RC_Title+RC_PARKING+RC_Room_Height+RC_RoomNumDemi+RC_RoomNumLarge+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX+RC_KITCHEN_FURN+RC_Appliances+RC_PARKING,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_Kermodel)


LM_Kermodel3<-train(RC_ArMil~MeretCleared+Kerulet+RC_Title,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_Kermodel3)

#leave RC_Condition behind
LM_Kermodel4<-train(RC_ArMil~MeretCleared*Kerulet+RC_Condition,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_Kermodel4)

LM_Kermodel4<-train(RC_ArMil~MeretCleared*Kerulet+RC_Condition,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_Kermodel4)

LM_Kermodel5<-train(RC_ArMil~MeretCleared*Kerulet+RC_Condition+RC_PARKING,data=TrainingSet,method="lm",metric="RMSE",trControl=TRC_CV10,model=T,na.action="na.exclude")
print(LM_Kermodel5)


TrainingSet1<-TrainingSet%>%add_residuals(LM_Kermodel5)

TrainingSet1<-TrainingSet1%>%add_predictions(LM_Kermodel5)
xyplot(TrainingSet1$RC_ArMil ~ TrainingSet1$pred, type = c("p", "g"), xlab = "Predicted", ylab = "Observed")

xyplot(TrainingSet1$resid ~ TrainingSet1$pred, type = c("p", "g"), xlab = "Predicted", ylab = "Residuals")

xyplot(TrainingSet1$resid ~ TrainingSet1$MeretCleared, type = c("p", "g"), xlab = "MeretCleared", ylab = "Residuals")
#xyplot(TrainingSet$RC_ArMil ~ predict(LM_Kermodel), type = c("p", "g"), xlab = "Predicted", ylab = "Observed")

#xyplot(resid(LM_Kermodel0) ~ predict(LM_Kermodel0), type = c("p", "g"), xlab = "Predicted", ylab = "Residuals")

}


rfModel0<-randomForest(RC_ArMil~MeretCleared+Kerulet,data=TrainingSet,ntrees = 1000)
print(rfModel0)

rfModel1<-randomForest(RC_ArMil~MeretCleared+Kerulet,data=TrainingSet,ntrees = 1000)
 print(rfModel1)

rfModel2<-randomForest(RC_ArMil~MeretCleared+Kerulet+RC_Title,data=TrainingSet,ntrees = 1000)
 print(rfModel2)
   
rfModel3<-randomForest(RC_ArMil~MeretCleared*(Kerulet)*RC_Title,data=TrainingSet,ntrees = 1000)
 print(rfModel3)
 
rfModel4<-randomForest(RC_ArMil~(MeretCleared+Kerulet)*RC_Title,data=TrainingSet,ntrees = 1000)

   
rfModelFull <-randomForest(RC_ArMil~MeretCleared+Kerulet+RC_Title+Kerulet+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX+RC_PARKING,data=TrainingSet,ntrees = 100)
print(rfModel)

#MP5
#modeltree neighbourhood
#Correlation coefficient                  0.8764
#Mean absolute error                      7.6151
#Root mean squared error                 13.5288
modelTree_RC_Neighborhood <- M5P(RC_ArMil ~ MeretCleared+RC_PARKING+RC_Condition+RC_STOREYREDUX+Kerulet+RC_Neighborhood, data = TrainingSet, control = Weka_control(M = 10))
summary(modelTree_RC_Neighborhood)
PredSet<-TrainingSet[,c('MeretCleared','Kerulet','RC_PARKING')]

modelTree_Kerulet<- M5P(RC_ArMil ~ PredSet, data = TrainingSet, control = Weka_control(M = 10))


#modeltree neighbourhood only 

#Correlation coefficient                  0.8187
#Mean absolute error                      8.8198
#Root mean squared error                 16.0843

modelTree_RC_Neighborhood_keruletOFF<- M5P(RC_ArMil ~ MeretCleared+RC_PARKING+RC_Condition+RC_STOREYREDUX+Location1, data = TrainingSet, control = Weka_control(M = 10))
summary(modelTree_RC_Neighborhood_keruletOFF)
PredSet<-TrainingSet[,c('MeretCleared','Kerulet','RC_PARKING')]



##12.404
modelTree1 <- M5P(RC_ArMil ~ MeretCleared+RC_Condition+Kerulet+RC_Material+RC_KITCHEN_FURN+RC_Room_Height+RC_PARKING+RC_STOREYREDUX+RC_Heating, data = TrainingSet, control = Weka_control(M = 10))

summary(modelTree1)
#12.11
modelTree2 <- M5P(RC_ArMil ~ MeretCleared+RC_Condition+Kerulet+RC_Material+RC_KITCHEN_FURN+Kertkapcsolatos+RC_Room_Height+RC_PARKING+RC_STOREYREDUX+RC_Heating, data = TrainingSet, control = Weka_control(M = 10))
summary(modelTree2)


#quadratic situation doesn't change the data 
modelTree3 <- M5P(RC_ArMil ~ MeretCleared+MeretCleared^2+RC_Condition+Kerulet+RC_Material+RC_KITCHEN_FURN+RC_Room_Height+RC_PARKING+RC_STOREYREDUX+RC_Heating, data = TrainingSet, control = Weka_control(M = 10))
summary(modelTree3)
# modelTree3 Root mean squared error                 11.8772 



    

modelTree4 <- M5P(RC_ArMil ~ MeretCleared+RC_Condition+Location1+RC_Material+RC_KITCHEN_FURN+Kertkapcsolatos+RC_Room_Height+RC_PARKING+RC_STOREYREDUX+RC_Heating, data = TrainingSet, control = Weka_control(M = 10))
summary(modelTree4)

#modelTree4
#=== Summary ===
#Correlation coefficient                  0.8961
#Mean absolute error                      6.5724
#Root mean squared error                 12.1009
#Relative absolute error                 36.9149 %
#Root relative squared error             44.3885 %
#Total Number of Instances            22921 



m5tree2 <- M5P(RC_ArMil ~ log(MeretCleared)+RC_Condition+RC_Material+RC_KITCHEN_FURN+RC_Room_Height+RC_PARKING+RC_STOREYREDUX+RC_Heating, data = TrainingSet, control = Weka_control(M = 10))
summary(m5tree2)
#m5tree2
#=== Summary === 
#Correlation coefficient                  0.8108
#Mean absolute error                      9.8242
#Root mean squared error                 16.2119
#Relative absolute error                 54.6564 %
#Root relative squared error             58.6526 %
#Total Number of Instances            27608  




TrainingSet1<-TrainingSet%>%add_residuals(m5tree1)

TrainingSet1<-TrainingSet1%>%add_predictions(m5tree1)


#RC_Title+Kerulet+RC_Neighborhood
#rfModel <-randomForest(RC_ArMil~MeretCleared+Kerulet+RC_Title+Location1+Kerulet+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX,data=TrainingSet,ntrees = 1000)
#print(rfModel)

#fit<-train(RC_ArMil~MeretCleared+Kerulet+RC_Material+RC_Room_Height+RC_Title+RC_Condition+RC_Heating+LocStreet1,data=ModelData,method="lm",metric="RMSE",trControl=control,na.action = "na.omit")
#print(fit)
```





In this section, I will start messing with the data manually. In the first part, I am going to build a linear regression model that uses the Kerulet(District) as its sole predictor.

Linear Regression is an exceptionally handy  regression type because it's interpretation of its coefficients are obvious. Unfortunately, it suffers from two majos showcomings:
1.It projects on a hyperplane, meaning meaning the unmodified function cannot map any curvature. 1.Secondly, LM treats outliers with equal weight.

1. Outliers:
Fitting of a linear regression model comes down minimimzing/maximizing the target metric (RMSE, R sq. for a the (X^t X)^-1 matrix. 

The maximisation methods are
1.RMSE 
1.Sum of absolute errors
1.HUbert'a method.(uses cutoff for mixing MSE and SAE)
SAE and HUbert's is more robust to outliers.(=less influental obs.)

2.To escape the hyperplane:
To fit to non-linear curves transformations like quadratic, cubic, interactions, additional predictors could be used.

The (X^t X)^-1 matrix is not unique when multicollinearity is present 2. the number of observation do not exceed the number of variables.

Correlated predictors diminish the effects each other, and greatly increase the standard error.

To combat this:
1. Remove the variables showing high pairwise collinearity
1. Use the collinearity inflation function to find instances where high covariance is shown between multiple vs 1 variable
1. Use Principal Component Analysis to reduce the number of variables (take heed that PCA doesn't care about the inferrability or the response variable and thus, it  may diminsh the predictive power of the model.)
1. Use PLS, ridge, lasso or  elastic net solutions to limit the number of variables in the model.


Remember! to take into account the decrease of observations available for training due to resampling!


Removing variables whose pairwise correlation is higher than a certain threshold let'say 0.9 

Successful fitting means that theresiduals do not show any correlations with the predicted values.

The PLS is a supervised dimension reduction technique that addresses the issue of lack of correlation witht the response variable by smultaneously maximising the covariance with the response  variable and the the variance of the predictors. 
(weight, scoes loadings)


PLS requires predictor to be scale and centered because it is drawn towards predictors with large variance.
pls has 1 tuningparameters:no variables to retain.



```{r model_building,purl=FALSE}
model_Meret<- lm(RC_ArMil ~ MeretCleared, data = TrainingSet)
coef(model_Meret)
grid <- TrainingSet %>%
  data_grid(MeretCleared)
grid<- grid %>% 
  add_predictions(model_Meret)

TrainingSet<-TrainingSet %>%
  add_residuals(model_Meret)

#as_tibble(AnalysisAdDetailList)

P1<-ggplot(model_Meret, aes( MeretCleared)) + 
  geom_point(aes(y = RC_ArMil)) +
  geom_line(aes(y = pred),data=grid, colour = "red", size = 1)
P2<-ggplot(TrainingSet, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)

grid.arrange(P1, P2, ncol=2)

```

In order to be able to choose between these tests, a metric for their efficiency needs to be determined. 

There are several metric for model evaluation, the general rule of thumb is that RMSE and R squared R squared for regressions and accuracy (and specificity), ROC(AUC), Cohen's Kappa and quasi R squared  for classification problems.

rsquared reflects the portion of the explained variance which means R squared is depends on the initial variance in the model. It is possible, thus, to acquirefantsticly high R sq values with relatively large  RMSE metrics.

```{r model_evaluation,purl=FALSE}
#lm model
TestSet$Pred_LM7 <- predict(LM_model7, TestSet)
AnalysisAdDetailList$PredictedValue<-predict(LM_model7, AnalysisAdDetailList)


#robust lm model
#RLM_model<-train(RC_ArMil~MeretCleared+RC_Title+Kerulet+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX,data=TrainingSet,method="lm",metric="RMSE",trControl=control,model=T)
lmValues1 <- data.frame(obs = TestSet$RC_ArMil, pred = TestSet$Pred_LM7)
defaultSummary(lmValues1)

#robust
#rlmPCA<-train(RC_ArMil~MeretCleared+RC_Title+Kerulet+RC_Condition+RC_Material+RC_Heating+RC_STOREYREDUX,data=TrainingSet,method="rlm",metric="RMSE",trControl=TRC_CV10,model=T)
```

```{r plotting,purl=FALSE}
xyplot(TrainingSet$RC_ArMil ~ predict(LM_model7), type = c("p", "g"), xlab = "Predicted", ylab = "Observed")
xyplot(resid(LM_model7) ~ predict(LM_model7), type = c("p", "g"), xlab = "Predicted", ylab = "Residuals")
```

```{r improvement_MLresults,purl=FALSE}
```

Use bagging or boosting to improve the well performing models. 
```{r ensemble_methods,purl=FALSE}
```


Use bagging or boosting tto imrpove the well performing models. 
```{r model_selection,purl=FALSE}

```

```{r Presentation,purl=FALSE}
flog.info("----- NEW EXECUTION COMPLETED-----")
```

#TODO

íadd projectTable <- Done 
ímegnézni, hogy miért nem lehet collapsolni a .rmd-t
Rendes loggolás
íreopened ADokat megnyitni. <-Done 
Megnézni,hogy a faktorizálás során lehet-e kódolni a kerületet factorra
íA lakás állapot NA-t stringgé alakítani (lásd guide)
íKódolni a lakás állapotát faktorra <-done
Kézi reportnál residualt gráfra
íAz árnál binnelni
íAz árnál az outliereket elengedni
Model hatékonyságot a másik blogról lecsenni
Ugyanezt LM után fákkal is megcsinálni

Faktoring típusú lakások kiszűrése kézzel. 
semi- supervised elemzés a faktoring típusú lakásokiszűrésére